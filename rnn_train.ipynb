{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'optimizer': 'rmsprop', 'neurons': 128, 'learning_rate': 0.001, 'init': 'glorot_uniform', 'epochs': 100, 'dropout_rate': 0.5, 'batch_size': 32}\n",
      "Best accuracy: 0.9786855578422546\n",
      "INFO:tensorflow:Assets written to: rnn_model\\assets\n",
      "Model trained and saved as rnn_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Preparation\n",
    "t_data = []\n",
    "t_labels = []\n",
    "\n",
    "folder = 'measurements'\n",
    "\n",
    "for i in range(1, 91):\n",
    "    f = os.path.join(folder, f'T_{i}.csv')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(f, header=None, delimiter=',', names=[0, 1, 2])\n",
    "    except pd.errors.ParserError:\n",
    "        try:\n",
    "            df = pd.read_csv(f, header=None, delimiter='\\t', names=[0, 1, 2])\n",
    "        except pd.errors.ParserError:\n",
    "            try:\n",
    "                df = pd.read_csv(f, header=None, delimiter=' ', names=[0, 1, 2])\n",
    "            except pd.errors.ParserError as e:\n",
    "                print(f\"Error reading {f}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    df = df.to_numpy()\n",
    "    df = pd.to_numeric(df.flatten(), errors='coerce').reshape(df.shape)\n",
    "\n",
    "    if not np.isfinite(df).all():\n",
    "        df[~np.isfinite(df)] = np.nan\n",
    "        imp = SI(strategy='mean')\n",
    "        df = imp.fit_transform(df)\n",
    "    \n",
    "    for j in range(df.shape[0]):\n",
    "        if (1 <= j+1 <= 101) or (104 <= j+1 <= 204) or (206 <= j+1 <= 308):\n",
    "            t_labels.append(1)\n",
    "            t_data.append(df[j])\n",
    "        elif (310 <= j+1 <= 411) or (413 <= j+1 <= 514) or (516 <= j+1 <= 618):\n",
    "            t_labels.append(0)\n",
    "            t_data.append(df[j])\n",
    "\n",
    "t_data = np.array(t_data)\n",
    "t_labels = np.array(t_labels)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "t_data = scaler.fit_transform(t_data)\n",
    "\n",
    "# Reshape data for RNN: (samples, time_steps, features)\n",
    "t_data = t_data.reshape((t_data.shape[0], 1, t_data.shape[1]))\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Define the RNN Model\n",
    "def create_model(optimizer='adam', init='he_uniform', dropout_rate=0.5, neurons=100, learning_rate=0.001):\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(neurons, activation='relu', kernel_initializer=init, input_shape=(1, t_data.shape[2])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons//2, activation='relu', kernel_initializer=init),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid', kernel_initializer=init)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_dist = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'init': ['he_uniform', 'glorot_uniform'],\n",
    "    'dropout_rate': [0.3, 0.5],\n",
    "    'neurons': [32, 64, 128],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1)\n",
    "random_result = random_search.fit(t_data, t_labels)\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters found: {random_result.best_params_}\")\n",
    "print(f\"Best accuracy: {random_result.best_score_}\")\n",
    "\n",
    "# Train the model\n",
    "best_model = random_result.best_estimator_.model\n",
    "\n",
    "# Save the trained model \n",
    "model_file = 'rnn_model'\n",
    "best_model.save(model_file, save_format='tf')\n",
    "\n",
    "print(f\"Model trained and saved as {model_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
